{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Wrappers introduction tutorial\n",
    "\n",
    "This tutorial is a \"how to\" or \"best practice\" around how most things are implemented or expected to be done using the neural wrappers library. The purpose of the library is to simplify training, testing and evaluation steps using the PyTorch library. It implements various helpful functions, such as `train_generator` (similar to a `fit_generator` Keras function), callbacks (`SaveModel`, `PlotMetrics` or custom ones by subclassing `Callback`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Model\n",
    "\n",
    "To load the library, all you have to do is subclass `NeuralNetworkPyTorch` instead of PyTorch's `nn.Module`. Basically the class is a subclass of `nn.Module` that has additional methods/fields inserted.\n",
    "\n",
    "`\n",
    "import torch.nn as nn\n",
    "class MyAwesomeNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyAwesomeNN, self).__init__()\n",
    "        ...\n",
    "->\n",
    "from neural_wrappers.pytorch import NeuralNetworkPyTorch\n",
    "class MyAwesomeNN(NeuralNetworkPyTorch):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model summary]\n",
      "General neural network architecture. Update __str__ in your model for more details when using summary.\n",
      "Parameters count: 0. Trainable parameters: 0.\n",
      "Hyperparameters: \n",
      "Metrics: 'Loss'\n",
      "Callbacks: None\n",
      "Optimizer: None\n",
      "Optimizer Scheduler: None\n",
      "GPU: False\n"
     ]
    }
   ],
   "source": [
    "from neural_wrappers.pytorch import NeuralNetworkPyTorch\n",
    "class MyAwesomeNN(NeuralNetworkPyTorch):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "model = MyAwesomeNN()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Data\n",
    "\n",
    "The built-in class that handles datasets is `DatasetReader`, however it is not necessary to use it, since any regular data generator will suffice. When creating a new reader, one must subclass this class and implement the dataset specific data loading parts. We'll take as example a simple logistic regression task with 3 classes, generated from a Gaussian distribution with various means/stds.\n",
    "\n",
    "The important methods are `generator = iterate_once(Type, miniBatchSize, ...)`, which returns a generator that can be directly fed to `train_generator(generator, numSteps, numEpochs, ...)`. The other important method is `numSteps = getNumIterations(Type, miniBatchSize)`, which returns the number of iterations per epoch.\n",
    "\n",
    "Type is simply a string, that is usually only one of 3 possible values (95% of times 2 values): `train`, `validation`, `test`.\n",
    "\n",
    "The naming convention is x (data), t (targets) and y (outputs). Thus a loss function would look like `y = Model(x); L = loss(y, t).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_wrappers.readers import DatasetReader\n",
    "from neural_wrappers.utilities import toCategorical\n",
    "import numpy as np\n",
    "\n",
    "class Reader(DatasetReader):\n",
    "    def __init__(self):\n",
    "        self.numClasses = 3\n",
    "        # This field is expected s.t. the default getNumIterations(...) method works. One can overwrite it however,\n",
    "        #  if a more special formula for computing the number of iterations per epoch is required.\n",
    "        self.numData = {\"train\" : 9999, \"validation\" : 3333, \"test\" : 999}\n",
    "        means = [-1, 0, 1.33]\n",
    "        stds = [0, 0.5, 0]\n",
    "        self.numFeatures = 5\n",
    "        \n",
    "        self.data = {k : self.generateData(k, means, stds) for k in self.numData.keys()}\n",
    "        print(\"Dataset:\", \" | \".join([\"%s : (X: %s, t: %s)\" % (k, self.data[k][0].shape, self.data[k][1].shape) \\\n",
    "                                      for k in self.data.keys()]) )\n",
    "              \n",
    "    def generateData(self, k, means, stds):\n",
    "        N = self.numData[k]\n",
    "        assert N % self.numClasses == 0\n",
    "        x = np.zeros((N, self.numFeatures), dtype=np.float32)\n",
    "        t = np.zeros((N, ), dtype=np.uint32)\n",
    "        for i in range(self.numClasses):\n",
    "            startIndex, endIndex = i * (N // 3), (i + 1) * (N // 3)\n",
    "            x[startIndex : endIndex] = np.random.randn(N // self.numClasses, self.numFeatures) * stds[i] + means[i]\n",
    "            t[startIndex : endIndex] = i\n",
    "        \n",
    "        # Shuffle the data so it's independent and identically distributed\n",
    "        np.random.seed(42)\n",
    "        perm = np.random.permutation(N)\n",
    "        x = x[perm]\n",
    "        # We also need to make the targets one hot encoded: [1] => [0, 1, 0] for 3 classes\n",
    "        t = t[perm]\n",
    "        t = toCategorical(t, numClasses=self.numClasses)\n",
    "        return x, t\n",
    "    \n",
    "    # This method is our generator, which must, at each step, return a batch of (X, t) items.\n",
    "    def iterate_once(self, type, miniBatchSize):\n",
    "        assert type in (\"train\", \"validation\", \"test\")\n",
    "\n",
    "        data = self.data[type]\n",
    "        x, t = data[0], data[1]\n",
    "        numIterations = self.getNumIterations(type, miniBatchSize)\n",
    "        for i in range(numIterations):\n",
    "            startIndex = i * miniBatchSize\n",
    "            endIndex = min((i + 1) * miniBatchSize, self.numData[type])\n",
    "            assert startIndex < endIndex, \"startIndex < endIndex. Got values: %d %d\" % (startIndex, endIndex)\n",
    "            yield x[startIndex : endIndex], t[startIndex : endIndex]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how a dataset generator should look like, let's play a little with the provided methods to see how they can help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train : (X: (9999, 5), t: (9999, 3)) | validation : (X: (3333, 5), t: (3333, 3)) | test : (X: (999, 5), t: (999, 3))\n",
      "Varying batch size:\n",
      "1 => 9999 | 2 => 5000 | 3 => 3333 | 5 => 2000 | 13 => 770 | 17 => 589 | 20 => 500 | \n",
      "Calling the generator and getting one batch\n",
      "First batch: (20, 5) [1 2 0 2 1 2 0 2 1 0 0 1 1 1 1 1 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "reader = Reader()\n",
    "## Vary the number of batch size\n",
    "print(\"Varying batch size:\")\n",
    "for MB in [1, 2, 3, 5, 13, 17, 20]:\n",
    "    trainNumSteps = reader.getNumIterations(\"train\", miniBatchSize=MB)\n",
    "    print(MB, \"=>\", trainNumSteps, end=\" | \")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Calling the generator and getting one batch\")\n",
    "generator = reader.iterate_once(\"train\", miniBatchSize=20)\n",
    "x, t = next(generator)\n",
    "print(\"First batch:\", x.shape, t.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Defining a model\n",
    "\n",
    "First, we define a model that takes the data (X), which has 5 features, and projects them into the 3 dimensional output, corresponding to the 3 classes. Then, we call the softmax function to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(NeuralNetworkPyTorch):\n",
    "    def __init__(self, numFeatures, outClasses):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=numFeatures, out_features=outClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model by generating some random data. The library provides a helpful wrapper on top of the forward function, called npForward, which take numpy arrays, converts them to torch tensors, calls the forward function and converts the result back to numpy arrays.\n",
    "\n",
    "Note that the returing torch arrays are detached from the original computational graph, so one cannot train a model like this. However `train_generator` takes care of this. `npForward` is useful for testing purposes mostly, after the model has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch result shape: torch.Size([37, 3])\n",
      "Numpy result shape: (37, 3)\n"
     ]
    }
   ],
   "source": [
    "model = Model(numFeatures=5, outClasses=3)\n",
    "\n",
    "MB = 37\n",
    "trX = tr.randn(MB, 5)\n",
    "trY = model.forward(trX)\n",
    "print(\"Torch result shape:\", trY.shape)\n",
    "\n",
    "npX = np.random.randn(MB, 5).astype(np.float32)\n",
    "npY = model.npForward(npX)\n",
    "print(\"Numpy result shape:\", npY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Training a Model\n",
    "\n",
    "The loss function is the negative log likelihood of the predicted probabilities against the expected one-hot encoded classes. This is standard multi class logistic regression.\n",
    "\n",
    "The class `NeuralNetworkPyTorch` expects to have a `criterion` set (loss function) and an `optimizer`, before being able to train the model on some data. These are set by `model.setCriterion(lossFn(y, t))` and `model.setOptimizer(optimizerType, **optimizerArgs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([1, 3])\n",
      "tensor([[0.1000, 0.5000, 0.4000]])\n",
      "tensor([[False,  True, False]])\n",
      "tensor(0.6931)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Negative log-likeklihood (used for softmax+NLL for classification), expecting targets are one-hot encoded\n",
    "def lossFn(y, t, debug=False):\n",
    "    # Outputs are transformed into probabilities using the softmax class\n",
    "    y = F.softmax(y, dim=1)\n",
    "\n",
    "    # Targets are transformed into one-hot encoding using the toCatergorical helpful function, then turned into booleans\n",
    "    #  for boolean indexing.\n",
    "    t = t.type(tr.bool)\n",
    "\n",
    "    if debug:\n",
    "        print(y)\n",
    "        print(t)\n",
    "\n",
    "    # This is simply boolean accessing the output probability softmax(y) of the correct target's index. So if we have\n",
    "    #  y = [0.1, 0.5, 0.4] and the target is [0, 1, 0], then we'll have y[t] == 0.5.\n",
    "    # Since our expected value of 1 (log(1) == 0), we have a loss of -log(0.5) = 0.693, which is minimized using the optimizer.\n",
    "    return (-tr.log(y[t] + 1e-5)).mean()\n",
    "\n",
    "y = tr.FloatTensor(np.log([[0.1, 0.5, 0.4]]))\n",
    "t = tr.from_numpy(toCategorical([[1]], numClasses=3))\n",
    "print(y.shape, t.shape)\n",
    "L = lossFn(y, t, debug=True)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model. One more trick we can use is calling the `getGenerators` function that is simply a wrapper on top of `iterate_once` and `getNumIterations` discussed earlier for the `DatasetReader` class. \n",
    "\n",
    "We'll also use some standard callbacks, such as plotting the loss and saving the model, which are added using the `addCallbacks` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Improvement (Loss) from nan to 0.24\n",
      "Epoch 2. Improvement (Loss) from 0.24 to 0.15\n",
      "Epoch 3. Improvement (Loss) from 0.15 to 0.11\n",
      "Epoch 4. Improvement (Loss) from 0.11 to 0.08\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from neural_wrappers.utilities import getGenerators\n",
    "from neural_wrappers.callbacks import SaveModels, PlotMetrics\n",
    "from neural_wrappers.metrics import Accuracy\n",
    "\n",
    "trainGenerator, trainSteps, valGenerator, valSteps = getGenerators(reader, miniBatchSize=20, keys=[\"train\", \"validation\"])\n",
    "\n",
    "model = Model(numFeatures=5, outClasses=3)\n",
    "model.setCriterion(lossFn)\n",
    "model.setOptimizer(optim.SGD, lr=0.01, momentum=0.5)\n",
    "model.addCallbacks([SaveModels(\"best\"), PlotMetrics([\"Loss\", \"Accuracy\"], [\"min\", \"max\"])])\n",
    "model.addMetrics({\"Accuracy\" : Accuracy()})\n",
    "\n",
    "# Train for 10 epochs using the defined data generators.\n",
    "model.train_generator(trainGenerator, trainSteps, 10, valGenerator, valSteps, printMessage=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Loading the stored model and testing\n",
    "By default models using the `SaveModels()` callback are stored under the name `model_best_<metric>.pkl`, in this case `model_best_Loss.pkl`. We'll load it using a new instance of the model and compute the loss on the test set. To load a model, we can use `loadWeights` or `loadModel`. The later one also loads the history of the model as well as optimizer hyperparameters and other info. It can be useful to continue a training loop that was stopped too soon by calling again `train_generator` with the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(numFeatures=5, outClasses=3)\n",
    "model.setCriterion(lossFn)\n",
    "model.addMetrics({\"Accuracy\" : Accuracy()})\n",
    "model.loadWeights(\"model_best_Loss.pkl\")\n",
    "\n",
    "testGenerator, testNumSteps = reader.iterate_once(\"test\", miniBatchSize=20), reader.getNumIterations(\"test\", miniBatchSize=20)\n",
    "metrics = model.test_generator(testGenerator, testNumSteps)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
