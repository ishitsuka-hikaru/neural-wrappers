# pytorch_wrapper Wrapper on top of the default pytorch network, which includes a method for automatic training
import torch as tr
import torch.nn as nn
import numpy as np
import joblib
import sys
from torch.autograd import Variable
from copy import copy

from metrics import Accuracy, Loss

def maybeCuda(x):
	return x.cuda() if tr.cuda.is_available() else x

def maybeCpu(x):
	return x.cpu() if tr.cuda.is_available() else x

# Labels can be None, in that case only data is available (testing cases without labels)
def makeGenerator(data, labels, batchSize):
	while True:
		numData = data.shape[0]
		numIterations = numData // batchSize + (numData % batchSize != 0)
		for i in range(numIterations):
			startIndex = i * batchSize
			endIndex = (i + 1) * batchSize
			if not labels is None:
				yield data[startIndex : endIndex], labels[startIndex : endIndex]
			else:
				yield data[startIndex : endIndex]

def getNumParams(params):
	numParams, numTrainable = 0, 0
	for param in params:
		npParamCount = np.prod(param.data.shape)
		numParams += npParamCount
		if param.requires_grad:
			numTrainable += npParamCount
	return numParams, numTrainable

# Wrapper on top of the PyTorch model. Added methods for saving and loading a state. To completly implement a PyTorch
#  model, one must define layers in the object's constructor, call setOptimizer, setCriterion and implement the
#  forward method identically like a normal PyTorch model.
class NeuralNetworkPyTorch(nn.Module):
	def __init__(self):
		self.optimizer = None
		self.optimizerStr = ""
		self.criterion = None
		self.metrics = {"Loss" : Loss()}
		# The hyperparameters are saved here (and should only be sent to constructor if they are to be included in the
		#  state of the model. Anything that happens in setup method, is about the learnable parameters s.a. weights,
		#  layers, etc)
		self.hyperParametersState = copy(vars(self))
		super(NeuralNetworkPyTorch, self).__init__()

	def setOptimizer(self, optimizerType, **kwargs):
		trainableParams = filter(lambda p : p.requires_grad, self.parameters())
		self.optimizer = optimizerType(trainableParams, **kwargs)

	def setOptimizerStr(self, Str):
		self.optimizerStr = Str

	def setCriterion(self, criterion):
		self.criterion = criterion

	def setMetrics(self, metrics):
		assert "Loss" in metrics, "At least one metric is required and Loss must be in them"
		if type(metrics) in (list, tuple):
			for metric in metrics:
				if metric == "Accuracy":
					self.metrics[metric] = Accuracy()
				elif metric == "Loss":
					self.metrics[metric] = Loss()
				else:
					raise NotImplementedError("Unknown metric provided: " + metric + ". Use dict and implementation")
		else:
			for key in metrics:
				if not type(key) is str:
					raise Exception("The key of the metric must be a string")
			self.metrics = metrics

	def summary(self):
		summaryStr = "[Model summary]\n"

		numParams, numTrainable = getNumParams(self.parameters())
		summaryStr += "Parameters count: %d. Trainable parameters: %d.\n" % (numParams, numTrainable)

		strMetrics = str(list(self.metrics.keys()))[1 : -1]
		summaryStr += "Metrics: %s\n" % (strMetrics)

		summaryStr += "Optimizer: %s\n" % (self.optimizerStr) if self.optimizerStr != None else ""

		# print("TODO more stuff...")
		# optimState = copy(self.optimizer.state_dict()["param_groups"])
		# if "params" in optimState:
		# 	del optimState["params"]
		# print("Optimizer state:", optimState)
		# print(self)

		return summaryStr

	# Handles all the initilization stuff of a specific pytorch model object.
	def setup(self):
		print("[NeuralNetworkPyTorch] Setup method is empty.")

	# Basic method that does a forward phase for one epoch given a generator. It can apply a step of optimizer or not.
	# @param[in] generator Object used to get a batch of data and labels at each step
	# @param[in] stepsPerEpoch How many items to be generated by the generator
	# @param[in] metrics A dictionary containing the metrics over which the epoch is run
	# @param[in] optimize If true, then the optimizer is also called after each iteration
	# @return The mean metrics over all the steps.
	def run_one_epoch(self, generator, stepsPerEpoch, optimize=False, printMessage=False):
		assert "Loss" in self.metrics.keys(), "At least one metric is required and Loss must be in them"
		metricResults = {metric : 0 for metric in self.metrics.keys()}

		for i, (npData, npLabels) in enumerate(generator):
			data = maybeCuda(Variable(tr.from_numpy(npData)))
			labels = maybeCuda(Variable(tr.from_numpy(npLabels)))

			results = self.forward(data)
			npResults = maybeCpu(results.data).numpy().reshape(npLabels.shape)
			
			loss = self.criterion(results, labels)
			npLoss = maybeCpu(loss.data).numpy()

			if optimize:
				self.optimizer.zero_grad()
				loss.backward()
				self.optimizer.step()

			for metric in self.metrics:
				metricResults[metric] += self.metrics[metric](npResults, npLabels, loss=npLoss)

			if printMessage:
				message = "Iteration: %d/%d." % (i + 1, stepsPerEpoch)
				for metric in metricResults:
					message += " %s: %2.2f." % (metric, metricResults[metric] / (i + 1))
				sys.stdout.write(message + "\r")
				sys.stdout.flush()

			del data, labels
			if i == stepsPerEpoch - 1:
				break

		for metric in metricResults:
			metricResults[metric] /= stepsPerEpoch
		return npResults, metricResults

	def train_generator(self, generator, stepsPerEpoch, numEpochs, callbacks=[], validationGenerator=None, \
		validationSteps=0):
		assert self.optimizer != None and self.criterion != None, "Set optimizer and criterion before training"
		sys.stdout.write("Training for %d epochs...\n" % (numEpochs))
		for epoch in range(numEpochs):
			done = (epoch + 1) / numEpochs * 100
			message = "Epoch %d/%d. Done: %2.2f%%." % (epoch + 1, numEpochs, done)

			# Run for training data and append the results
			_, trainMetrics = self.run_one_epoch(generator, stepsPerEpoch, optimize=True, printMessage=True)
			for metric in trainMetrics:
				message += " %s: %2.2f." % (metric, trainMetrics[metric])

			# Run for validation data and append the results
			if validationGenerator != None:
				_, validationMetrics = \
					self.run_one_epoch(validationGenerator, validationSteps, optimize=False, printMessage=False)
				for metric in validationMetrics:
					message += " %s: %2.2f." % ("Val " + metric, validationMetrics[metric])

			sys.stdout.write(message + "\n")
			sys.stdout.flush()

			# Do the callbacks
			callbackArgs = {
				"model" : self,
				"epoch" : epoch + 1,
				"numEpochs" : numEpochs,
				"trainMetrics" : trainMetrics,
				"validationMetrics" : validationMetrics if validationGenerator != None else None
			}
			for callback in callbacks:
				callback(**callbackArgs)

	def train_model(self, data, labels, batchSize, numEpochs, callbacks=[], validationData=None, \
		validationLabels=None):
		assert self.optimizer != None and self.criterion != None, "Set optimizer and criterion before training"
		dataGenerator = makeGenerator(data, labels, batchSize)
		numIterations = data.shape[0] // batchSize + (data.shape[0] % batchSize != 0)

		validationGenerator = makeGenerator(validationData, validationLabels, validationData.shape[0]) if \
			not validationLabels is None else None

		self.train_generator(dataGenerator, stepsPerEpoch=numIterations, numEpochs=numEpochs, callbacks=callbacks, \
			validationGenerator=validationGenerator, validationSteps=1)

	# def test_generator(self, dataGenerator, numIterations):
	# 	return self.run_one_epoch(dataGenerator, numIterations)

	# # labels can be None if not available
	# def test(self, data, labels, batchSize):
	# 	testGenerator = makeGenerator(test, labels, batchSize)
	# 	numIterations = data.shape[0] // batchSize + (data.shape[0] % batchSize != 0)
	# 	return self.run_one_epoch(testGenerator, numIterations, optimize=False, printMessage=False)

	def save_weights(self, path):
		modelParams = list(map(lambda x : x.cpu(), self.parameters()))
		tr.save(modelParams, path)

	def load_weights(self, path):
		params = tr.load(path)
		loadedParams, _ = getNumParams(params)
		thisParams, _ = getNumParams(self.parameters())
		if loadedParams != thisParams:
			raise Exception("Inconsistent parameters: %d vs %d." % (loadedParams, thisParams))

		for i, item in enumerate(self.parameters()):
			if item.data.shape != params[i].data.shape:
				raise Exception("Inconsistent parameters: %d vs %d." % (item.data.shape, params[i].data.shape))
			item.data = maybeCuda(params[i].data)

	# Method used to save the current state of the model (any hyperparameters that are not trainable, but fixed at
	#  train time and needed at test time).
	def save_state(self, path):
		print("Saving the hyperparamters state of the model to", path)
		joblib.dump(self.hyperParametersState, path)

	# Used to load the state of this object from a previous saved file.
	def load_state(self, path):
		print("Loading the state of the model from", path)
		state = joblib.load(path)
		for key in state:
			setattr(self, key, state[key])
			self.hyperParametersState[key] = state[key]
		self.setup()
