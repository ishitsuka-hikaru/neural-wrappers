import torch as tr
import torch.nn as nn
from datetime import datetime
from functools import partial
from overrides import overrides
from copy import copy

from ..pytorch import FeedForwardNetwork, npGetData, trGetData, npToTrCall, trToNpCall
from .utils import MessagePrinter, getFormattedStr
from .draw_graph import drawGraph
from ..callbacks import CallbackName
from .graph_serializer import GraphSerializer

class Graph(FeedForwardNetwork):
	def __init__(self, edges, hyperParameters={}):
		self.edges =edges
		self.nodes = self.getNodes()
		hyperParameters = self.getHyperParameters(hyperParameters)
		super().__init__(hyperParameters=hyperParameters)

		self.edges = nn.ModuleList(self.getEdges())
		self.edgeIDsToEdges = self.getStrMapping()
		self.edgeLoss = {}
		self.linePrinter = MessagePrinter(None)
		self.setCriterion(self.loss)
		self.serializer = GraphSerializer(self)
		self.iterPrintMessageKeys = [CallbackName("Loss")]

	def loss(self, y, t):
		loss = 0
		for edge in self.edges:
			edgeID = str(edge)
			edgeLoss = edge.loss(y, t)
			self.edgeLoss[edgeID] = npGetData(edgeLoss)
	
			# If this edge has no loss, ignore it.
			if edgeLoss is None:
				continue
			# If this edge is not trainable, also ignore it (? To think if this is correct ?)
			# TODO: see how to fast check if edge is trainable (perhaps not an issue at all to add untrainable ones)

			# Otherwise, just add it to the loss of the entire graph
			loss += edgeLoss
		return loss

	# Graphs and subgraphs use all the possible inputs.
	# TODO: Perhaps it'd be better to check what inputs the edges require beforehand, but that might be just too
	#  and redundant, since the forward of the subgraphs will call getInputs of each edge anyway.
	def getInputs(self, trInputs):
		return trInputs

	def forward(self, trInputs):
		trResults = {}
		# TODO: Execution order. (synchronus vs asynchronus as well as topological sort at various levels.)
		# For now, the execution is synchronous and linear as defined by the list of edges
		for edge in self.edges:
			edgeID = str(edge)
			edgeInputs = edge.getInputs(trInputs)
			edgeOutput = edge.forward(edgeInputs)
			# Update the outputs of the whole graph as well
			trResults[edgeID] = edgeOutput
		return trResults

	def getEdges(self):
		edges = []
		for edge in self.edges:
			edges.append(edge)
		return edges

	def getStrMapping(self):
		res = {}
		for edge in self.edges:
			edgeMapping = edge.getStrMapping()
			# This adds graphs too
			res[str(edge)] = edge
			if type(edgeMapping) == str:
				res[edgeMapping] = edge
			else:
				for k in edgeMapping:
					res[k] = edgeMapping[k]
		return res

	def getNodes(self):
		nodes = set()
		for edge in self.edges:
			# edge can be an actual Graph.
			for node in edge.getNodes():
				nodes.add(node)
		return nodes

	def initializeEpochMetrics(self):
		res = super().initializeEpochMetrics()
		for edge in self.edges:
			res[str(edge)] = edge.initializeEpochMetrics()
		return res

	def reduceEpochMetrics(self, metricResults):
		results = super().reduceEpochMetrics(metricResults)
		for edge in self.edges:
			results[str(edge)] = edge.reduceEpochMetrics(metricResults[str(edge)])
		return results

	### Some updates to original NeuralNetworkPyTorch to work seamlessly with graphs (mostly printing)
	
	def mainLoop(self, npInputs, npLabels, isTraining=False, isOptimizing=False):
		trInputs, trLabels = trGetData(npInputs), trGetData(npLabels)
		self.iterationEpilogue(isTraining, isOptimizing, trLabels)

		# Call the network algorithm. By default this is just results = self.forward(inputs);
		#  loss = criterion(results). But this can be updated for specific network architectures (i.e. GANs)
		trResults, trLoss = self.networkAlgorithm(trInputs, trLabels, isTraining, isOptimizing)

		npResults, npLoss = npGetData(trResults), npGetData(trLoss)

		# Might be better to use a callback so we skip this step
		if not trLoss is None:
			if isTraining and isOptimizing:
				self.optimizer.zero_grad()
				trLoss.backward()
				self.optimizer.step()
			else:
				trLoss.detach_()

		return npResults, npLoss

	# Basic method that does a forward phase for one epoch given a generator. It can apply a step of optimizer or not.
	# @param[in] generator Object used to get a batch of data and labels at each step
	# @param[in] stepsPerEpoch How many items to be generated by the generator
	# @param[in] metrics A dictionary containing the metrics over which the epoch is run
	# @return The mean metrics over all the steps.
	def run_one_epoch(self, generator, stepsPerEpoch, isTraining, isOptimizing, **kwargs):
		assert stepsPerEpoch > 0
		if isOptimizing == False and tr.is_grad_enabled():
			print("Warning! Not optimizing, but grad is enabled.")
		if isTraining and isOptimizing:
			assert not self.optimizer is None, "Set optimizer before training"
		assert not self.criterion is None, "Set criterion before training or testing"
		metricResults = self.initializeEpochMetrics()
		self.linePrinter = MessagePrinter(kwargs["printMessage"])

		# The protocol requires the generator to have 2 items, inputs and labels (both can be None). If there are more
		#  inputs, they can be packed together (stacked) or put into a list, in which case the ntwork will receive the
		#  same list, but every element in the list is tranasformed in torch format.
		startTime = datetime.now()
		for i, items in enumerate(generator):
			npInputs, npLabels = items
			npResults, npLoss = self.mainLoop(npInputs, npLabels, isTraining, isOptimizing)

			self.iterationPrologue(npInputs, npLabels, npResults, npLoss, i, stepsPerEpoch, \
				metricResults, isTraining, isOptimizing, startTime)

			if i == stepsPerEpoch - 1:
				break

		if i != stepsPerEpoch - 1:
			self.linePrinter(("Warning! Number of iterations (%d) does not match expected ") + \
				("iterations in reader (%d)") % (i, stepsPerEpoch - 1))

		res = self.reduceEpochMetrics(metricResults)
		return res

	def getGroundTruth(self, x):
		return x

	def iterationPrologue(self, inputs, labels, results, loss, iteration, \
		stepsPerEpoch, metricResults, isTraining, isOptimizing, startTime):
		# metrics and callbacks are merged. Each callback/metric can have one or more "parents" which
		#  forms an ayclical graph. They must be called in such an order that all the parents are satisfied before
		#  all children (topological sort).
		# Iteration callbacks are called here. These include metrics or random callbacks such as plotting results
		#  in testing mode.
		self.callbacksOnIterationEnd(data=inputs, labels=labels, results=results, \
			loss=loss, iteration=iteration, numIterations=stepsPerEpoch, metricResults=metricResults, \
			isTraining=isTraining, isOptimizing=isOptimizing)

		# Print the message, after the metrics are updated.
		iterFinishTime = (datetime.now() - startTime)
		message = self.computeIterPrintMessage(iteration, stepsPerEpoch, metricResults, iterFinishTime)
		self.linePrinter(message)

	def callbacksOnIterationEnd(self, data, labels, results, loss, iteration, numIterations, \
		metricResults, isTraining, isOptimizing):
		thisResults = super().callbacksOnIterationEnd(data, labels, results, loss, iteration, numIterations, \
				metricResults, isTraining, isOptimizing)

		for edge in self.edges:
			edgeResults = results[str(edge)]
			edgeLabels = edge.getGroundTruth(labels)
			edgeMetricResults = metricResults[str(edge)]
			edgeLoss = self.edgeLoss[str(edge)]
			thisResults[str(edge)] = edge.callbacksOnIterationEnd(data, edgeLabels, \
				edgeResults, edgeLoss, iteration, numIterations, edgeMetricResults, isTraining, isOptimizing)
		return thisResults

	def metricsSummary(self):
		summaryStr = super().metricsSummary()
		for edge in self.edges:
			strEdge = str(edge)
			if type(edge) == Graph:
				strEdge = "SubGraph"
			lines = edge.metricsSummary().split("\n")[0 : -1]
			if len(lines) > 0:
				summaryStr += "\t- %s:\n" % (strEdge)
				for line in lines:
					summaryStr += "\t%s\n" % (line)
		return summaryStr

	def networkComputeIterPrintMessage(self, i, stepsPerEpoch, metricResults, iterFinishTime, iterPrintMessageKeys):
		messages = []
		message = "Epoch: %d. Iteration: %d/%d." % (self.currentEpoch, i + 1, stepsPerEpoch)
		# iterFinishTime / (i + 1) is the current estimate per iteration. That value times stepsPerEpoch is
		#  the current estimation per epoch. That value minus current time is the current estimation for
		#  time remaining for this epoch. It can also go negative near end of epoch, so use abs.
		ETA = abs(iterFinishTime / (i + 1) * stepsPerEpoch - iterFinishTime)
		message += " ETA: %s" % (ETA)
		messages.append(message)

		if self.optimizer:
			messages.append("  - Optimizer: %s" % (getOptimizerStr(self.optimizer)))

		message = "  - Metrics."
		metricKeys = filter(lambda x : isinstance(x, CallbackName), metricResults.keys())
		metricKeys = sorted(list(set(metricKeys)))
		Keys = list(filter(lambda x : x in iterPrintMessageKeys, metricKeys))
		for key in Keys:
			# X = metricResults[key].get()
			# breakpoint()
			formattedStr = getFormattedStr(metricResults[key].get(), precision=3)
			message += " %s: %s." % (key, formattedStr)
		if len(Keys) > 0:
			messages.append(message)
		return messages

	def computeIterPrintMessage(self, i, stepsPerEpoch, metricResults, iterFinishTime):
		messages = self.networkComputeIterPrintMessage(i, stepsPerEpoch, metricResults, \
			iterFinishTime, self.iterPrintMessageKeys)

		for edge in self.edges:
			if type(edge) == Graph:
				strEdge = "SubGraph"
			else:
				strEdge = str(edge)
			edgeMetrics = metricResults[str(edge)]
			if len(edgeMetrics) == 0:
				continue
			edgeIterPrintMessage = self.networkComputeIterPrintMessage(i, stepsPerEpoch, \
				edgeMetrics, iterFinishTime, edge.iterPrintMessageKeys)[1 :]
			messages.append(strEdge)
			messages.extend(edgeIterPrintMessage)
		return messages

	# Computes the message that is printed to the stdout. This method is also called by SaveHistory callback.
	# @param[in] kwargs The arguments sent to any regular callback.
	# @return A string that contains the one-line message that is printed at each end of epoch.
	def computePrintMessage(self, trainMetrics, validationMetrics, numEpochs, duration):
		messages = super().computePrintMessage(trainMetrics, validationMetrics, numEpochs, duration)
		for edge in self.edges:
			if type(edge) == Graph:
				strEdge = "SubGraph"
			else:
				strEdge = str(edge)
			edgeTrainMetrics = trainMetrics[str(edge)]
			edgeValMetrics = validationMetrics[str(edge)]
			if len(edgeTrainMetrics) == 0:
				continue
			edgePrintMessage = edge.computePrintMessage(edgeTrainMetrics, edgeValMetrics, numEpochs, duration)[1 : ]
			messages.append(strEdge)
			messages.extend(edgePrintMessage)
		return messages

	def iterationEpilogue(self, isTraining, isOptimizing, trLabels):
		# Set the GT for each node based on the inputs available at this step. Edges may overwrite this when reaching
		#  a node via an edge, however it is the graph's responsability to set the default GTs. What happens during the
		#  optimization shouldn't be influenced by this default.
		# If the ground truth key is "*", then all items are provided to the node and it's expected that the node will
		#  manage the labels accordingly.
		for node in self.nodes:
			node.setGroundTruth(trLabels)
			node.messages = {}

	def draw(self, fileName, cleanup=True, view=False):
		drawGraph(self.nodes, self.edges, fileName, cleanup, view)

	def getHyperParameters(self, hyperParameters):
		# Set up hyperparameters for every node
		hyperParameters = {k : hyperParameters[k] for k in hyperParameters}
		for node in self.nodes:
			hyperParameters[node.name] = node.hyperParameters
		for edge in self.edges:
			hyperParameters[str(edge)] = edge.hyperParameters
		return hyperParameters

	def graphStr(self, depth=1):
		Str = "Graph:"
		pre = "\t" * depth
		for edge in self.edges:
			if type(edge) == Graph:
				edgeStr = edge.graphStr(depth + 1)
			else:
				edgeStr = str(edge)
			Str += "\n%s-%s" % (pre, edgeStr)
		return Str

	def __str__(self):
		return self.graphStr()