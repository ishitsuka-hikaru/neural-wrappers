import torch as tr
import numpy as np

from .network import NeuralNetworkPyTorch
from .pytorch_utils import maybeCuda, maybeCpu
from inspect import signature
from datetime import datetime

class RecurrentNeuralNetworkPyTorch(NeuralNetworkPyTorch):
	def __str__(self):
		return "General recurrent neural network architecture. Update __str__ in your model for more details when " + \
			"using summary."

	# Basic method that does a forward phase for one epoch given a generator modified for recurrent neural networks. It
	#  can apply a step of optimizer or not.
	# @param[in] generator Object used to get a batch of data and labels at each step
	# @param[in] stepsPerEpoch How many items to be generated by the generator
	# @param[in] metrics A dictionary containing the metrics over which the epoch is run
	# @return The mean metrics over all the steps.
	def run_one_epoch(self, generator, stepsPerEpoch, callbacks=[], printMessage=False):
		if tr.is_grad_enabled():
			assert not self.optimizer is None, "Set optimizer before training"
		assert not self.criterion is None, "Set criterion before training or testing"
		assert "Loss" in self.metrics.keys(), "Loss metric was not found in metrics."
		self.checkCallbacks(callbacks)
		self.callbacksOnEpochStart(callbacks)

		metricResults = {metric : 0 for metric in self.metrics.keys()}
		i = 0

		self.setTrainable(tr.is_grad_enabled())
		if tr.is_grad_enabled():
			optimizeCallback = (lambda optim, loss : (optim.zero_grad(), loss.backward(), optim.step()))
		else:
			optimizeCallback = (lambda optim, loss : loss.detach_())

		startTime = datetime.now()
		for i, (npData, npLabels) in enumerate(generator):
			trData = tr.from_numpy(npData)
			trLabels = tr.from_numpy(npLabels)

			# Each timestep is sent manually (instead of letting pytorch do the loop itself). The shape of trData
			#  should be: MB x T x dataShape. This is needed since there may be other network components that do not
			#  with time series (such as Conv2Ds), and we take the pressure off the model so implement just the simple
			#  T=1 case.
			# The sequence size is extracted from the returned data. Thus, the data INSIDE the minibatch must be
			#  identical w.r.t timestamps. If the data timestamps is different, they must be padded inside the
			#  generator to be identical.
			sequenceSize = npData.shape[1]

			# Do the first timestamp separate, so we can infer the result shape
			data = maybeCuda(trData[:, 0])
			labels = maybeCuda(trLabels[:, 0])
			# For the hiden states, use a list of Nones, expecting the signature to be: (x, hidden1, ..., hiddenN)
			initialHiddenState = [None] * (len(signature(self.forward).parameters) - 1)
			results, hiddenState = self.forward(data, *initialHiddenState)
			npResult = maybeCpu(results.detach()).numpy()
			npResults = np.zeros((npData.shape[0], npData.shape[1], *npResult.shape[1 : ]))
			npResults[:, 0] = npResult
			loss = self.criterion(results, labels)

			# Then the next N - 1 timesteps are done assuming that the shape of the result doensn't change.
			for t in range(1, sequenceSize):
				# Slicing the data as: MB x 1 x dataShape, sending each sequence one by one.
				data = maybeCuda(trData[:, t])
				labels = maybeCuda(trLabels[:, t])

				results, hiddenState = self.forward(data, hiddenState)
				result = maybeCpu(results.detach()).numpy()
				npResults[:, t] = result

				loss += self.criterion(results, labels)
			npLoss = maybeCpu(loss.detach()).numpy()
			optimizeCallback(self.optimizer, loss)

			# Iteration callbacks are called here (i.e. for plotting results!)
			self.callbacksOnIterationEnd(callbacks, data=npData, labels=npLabels, results=npResults, loss=npLoss, \
				iteration=i, numIterations=stepsPerEpoch, hiddenState=hiddenState)

			for metric in self.metrics:
				metricResults[metric] += self.metrics[metric](npResults, npLabels, loss=npLoss)

			iterFinishTime = (datetime.now() - startTime)
			if printMessage:
				self.linePrinter.print(self.computeIterPrintMessage(i, stepsPerEpoch, metricResults, iterFinishTime))

			del data, labels
			if i == stepsPerEpoch - 1:
				break

		if i != stepsPerEpoch - 1:
			sys.stderr.write("Warning! Number of iterations (%d) does not match expected iterations in reader (%d)" % \
				(i, stepsPerEpoch - 1))
		for metric in metricResults:
			metricResults[metric] /= stepsPerEpoch
		return metricResults