import numpy as np
from prefetch_generator import BackgroundGenerator
from neural_wrappers.transforms import Transformer
from neural_wrappers.utilities import standardizeData, minMaxNormalizeData
from functools import partial

# TODO: use datasetDimensions and labelDimensions for multiple dimensions from each h5py file.

class DatasetReader:
	def __init__(self, datasetPath, dataShape, labelShape=None, transforms=["none"], \
		normalization="standardization"):
		self.datasetPath = datasetPath
		self.dataShape = dataShape
		self.labelShape = labelShape
		self.transforms = transforms
		self.normalization = normalization

		self.dataAugmenter = Transformer(transforms, dataShape=dataShape, labelShape=labelShape)
		self.validationAugmenter = Transformer(["none"], dataShape=dataShape, labelShape=labelShape)

		if normalization == "min_max_normalization":
			self.normalizer = self.minMaxNormalizer
		elif normalization == "standardization":
			self.normalizer = self.standardizer
		elif normalization == "none":
			self.normalizer = lambda data, type: data
		else:
			assert hasattr(normalization, "__call__"), "The user provided normalization must be callable or must " + \
				"one of \"standardization\", \"min_max_normalization\", \"none\""
			self.normalizer = partial(normalization, obj=self)

	# @brief Basic min max normalizer, which receives a batches data (MB x shape) and applies the normalization for
	#  each dimension independelty. Requires the class members minimums and maximums to be defined inside the class
	#  for this normalization to work.
	# @param[in] data The data on which the normalization is applied
	# @param[in] type The type (data dimension) for which the field minimums and maximums are searched into
	def minMaxNormalizer(self, data, type):
		data = np.float32(data)
		if self.numDimensions[type] == 1:
			data = minMaxNormalizeData(data, self.minimums[type], self.maximums[type])
		else:
			for i in range(self.numDimensions[type]):
				data[..., i] = minMaxNormalizeData(data[..., i], self.minimums[type][i], self.maximums[type][i])
		return data

	# @brief Basic standardization normalizer, using same convention as minMaxNormalizer.
	# @param[in] data The data on which the normalization is applied
	# @param[in] type The type (data dimension) for which the field means and stsd are searched into
	def standardizer(self, data, type):
		data = np.float32(data)
		if self.numDimensions[type] == 1:
			data = standardizeData(data, self.means[type], self.stds[type])
		else:
			for i in range(self.numDimensions[type]):
				data[..., i] = standardizeData(data[..., i], self.means[type][i], self.stds[type][i])
		return data

	# Handles all the initilization stuff of a specific dataset object.
	def setup(self):
		raise NotImplementedError("Should have implemented this")

	# Generic generator function, which should iterate once the dataset and yield a minibatch subset every time
	def iterate_once(self, type, miniBatchSize):
		raise NotImplementedError("Should have implemented this")

	# Computes the class members indexes and numData, which represent the amount of data of each portion of the
	#  datset (train/test/val) as well as the starting indexes
	def computeIndexesSplit(self, numAllData):
		# Check validity of the dataSplit (sums to 100 and positive)
		assert len(self.dataSplit) == 3 and self.dataSplit[0] >= 0 and self.dataSplit[1] >= 0 \
			and self.dataSplit[2] >= 0 and self.dataSplit[0] + self.dataSplit[1] + self.dataSplit[2] == 100

		trainStartIndex = 0
		testStartIndex = self.dataSplit[0] * numAllData // 100
		validationStartIndex = testStartIndex + (self.dataSplit[1] * numAllData // 100)

		indexes = {
			"train" : (trainStartIndex, testStartIndex),
			"test" : (testStartIndex, validationStartIndex),
			"validation" : (validationStartIndex, numAllData)
		}

		numSplitData = {
			"train" : testStartIndex,
			"test" : validationStartIndex - testStartIndex,
			"validation" : numAllData - validationStartIndex
		}
		return indexes, numSplitData

	# Generic infinite generator, that simply does a while True over the iterate_once method, which only goes one epoch
	# @param[in] type The type of processing that is generated by the generator (typicall train/test/validation)
	# @param[in] miniBatchSize How many items are generated at each step
	# @param[in] maxPrefetch How many items in advance to be generated and stored before they are consumed. If 0, the
	#  thread API is not used at all. If 1, the thread API is used with a queue of length 1 (still works better than
	#  normal in most cases, due to the multi-threaded nature. For length > 1, the queue size is just increased.
	def iterate(self, type, miniBatchSize, maxPrefetch=0):
		assert maxPrefetch >= 0
		while True:
			iterateGenerator = self.iterate_once(type, miniBatchSize)
			if maxPrefetch > 0:
				iterateGenerator = BackgroundGenerator(iterateGenerator, max_prefetch=maxPrefetch)
			for items in iterateGenerator:
				yield items
				del items

	# Finds the number of iterations needed for each type, given a miniBatchSize. Eachs transformations adds a new set
	#  of parameters. If none are present then just one set of parameters 
	# @param[in] type The type of data from which this is computed (e.g "train", "test", "validation")
	# @param[in] miniBatchSize How many data from all the data is taken at every iteration
	# @param[in] accountTransforms Take into account transformations or not. True value is used in neural_network
	#  wrappers, so if there are 4 transforms, the amount of required iterations for one epoch is numData * 4.
	#  Meanwhile, in reader classes, all transforms are done in the same loop (see NYUDepthReader), so these all
	#  represent same epoch. Defaults to True, so end-users when training networks aren't required to specify it.
	def getNumIterations(self, type, miniBatchSize, accountTransforms=True):
		N = self.numData[type] // miniBatchSize + (self.numData[type] % miniBatchSize != 0)
		assert len(self.transforms), "No transforms used, perhaps set just \"none\""
		return N if accountTransforms == False else N * len(self.transforms)

	# Returns the mean of the dataset. Sometimes differnt means are used for different shapes (due to rescaling).
	def getMean(self, shape=None):
		raise NotImplementedError("Should have implemented this")

	def getStd(self, shape=None):
		raise NotImplementedError("Should have implemented this")

	def __str__(self):
		return "General dataset reader. Update __str__ in your dataset for more details when using summary."

	def summary(self):
		summaryStr = "[Dataset summary]\n"
		summaryStr += self.__str__() + "\n"

		summaryStr += "Num data: %s\n" % (self.numData)
		summaryStr += "Transforms(%i): %s\n" % (len(self.transforms), self.transforms)
		return summaryStr

class ClassificationDatasetReader(DatasetReader):
	# Classification problems are split into N classes which varies from data to data.
	def getNumberOfClasses(self):
		raise NotImplementedError("Should have implemented this")

	def getClasses(self):
		raise NotImplementedError("Should have implemented this")
