import torch as tr
import numpy as np
from .network import NeuralNetworkPyTorch
from torch.autograd import Variable

class RecurrentNeuralNetworkPyTorch(NeuralNetworkPyTorch):
	# Basic method that does a forward phase for one epoch given a generator modified for recurrent neural networks. It
	#  can apply a step of optimizer or not.
	# @param[in] generator Object used to get a batch of data and labels at each step
	# @param[in] stepsPerEpoch How many items to be generated by the generator
	# @param[in] metrics A dictionary containing the metrics over which the epoch is run
	# @param[in] optimize If true, then the optimizer is also called after each iteration
	# @return The mean metrics over all the steps.
	def run_one_epoch(self, generator, stepsPerEpoch, callbacks=[], optimize=False, printMessage=False, debug=False):
		assert "Loss" in self.metrics.keys(), "At least one metric is required and Loss must be in them"
		assert not self.criterion is None, "Expected a criterion/loss to be set before training/testing."
		metricResults = {metric : 0 for metric in self.metrics.keys()}

		for i, (npData, npLabels) in enumerate(generator):
			loss = 0
			hiddenState = DummyIter()

			trData = tr.from_numpy(npData)
			trLabels = tr.from_numpy(npLabels)
			npResults = np.zeros(npLabels.shape)

			# Each timestep is sent manually (instead of letting pytorch do the loop itself). The shape of trData
			#  should be: MB x T x dataShape. This is needed since there may be other network components that do not
			#  with time series (such as Conv2Ds), and we take the pressure off the model so implement just the simple
			#  T=1 case.
			# The sequence size is extracted from the returned data. Thus, the data INSIDE the minibatch must have
			#  identical w.r.t timestamps. If the data timestamps is different, they must be padded inside the
			#  generator to be identical.
			sequenceSize = npData.shape[1]

			for t in range(sequenceSize):
				# Slicing the data as: MB x 1 x dataShape, sending each sequence one by one.
				data = maybeCuda(Variable(trData[:, t]))
				labels = maybeCuda(Variable(trLabels[:, t]))

				results, hiddenState = self.forward(data, hiddenState)
				npResults[:, t] = maybeCpu(results.data).numpy()

				loss += self.criterion(results, labels)
			npLoss = maybeCpu(loss.data).numpy()
			if debug:
				print("\nLoss: %2.6f" % (npLoss))

			if optimize:
				self.optimizer.zero_grad()
				loss.backward()
				self.optimizer.step()

			# Iteration callbacks are called here (i.e. for plotting results!)
			callbackArgs = {
				"data" : npData,
				"labels" : npLabels,
				"results" : npResults,
				"loss" : npLoss,
				"hiddenState" : hiddenState
			}
			for callback in callbacks:
				callback(**callbackArgs)

			for metric in self.metrics:
				metricResults[metric] += self.metrics[metric](npResults, npLabels, loss=npLoss)

			if printMessage:
				message = "Iteration: %d/%d." % (i + 1, stepsPerEpoch)
				for metric in metricResults:
					message += " %s: %2.2f." % (metric, metricResults[metric] / (i + 1))
				sys.stdout.write(message + "\r")
				sys.stdout.flush()

			del data, labels
			if i == stepsPerEpoch - 1:
				break

		for metric in metricResults:
			metricResults[metric] /= stepsPerEpoch
		return metricResults